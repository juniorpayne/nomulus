<?xml version="1.0" encoding="UTF-8"?>
<cronentries>

  <!--
    /cron/fanout params:
        queue=<QUEUE_NAME>
        endpoint=<ENDPOINT_NAME> // URL Path of servlet, which may contain placeholders:
        runInEmpty               // Run once, with no tld parameter
        forEachRealTld           // Run for tlds with getTldType() == TldType.REAL
        forEachTestTld           // Run for tlds with getTldType() == TldType.TEST
        exclude=TLD1[,TLD2]      // exclude something otherwise included
   -->

  <cron>
    <url>/_dr/task/rdeStaging</url>
    <description>
      This job generates a full RDE escrow deposit as a single gigantic XML document
      and streams it to cloud storage. When this job has finished successfully, it'll
      launch a separate task that uploads the deposit file to Iron Mountain via SFTP.
    </description>
    <schedule>every day 00:07</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=rde-upload&endpoint=/_dr/task/rdeUpload&forEachRealTld]]></url>
    <description>
      This job is a no-op unless RdeUploadCursor falls behind for some reason.
    </description>
    <schedule>every 4 hours synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=marksdb&endpoint=/_dr/task/tmchDnl&runInEmpty]]></url>
    <description>
      This job downloads the latest DNL from MarksDB and inserts it into the database.
      (See: TmchDnlServlet, ClaimsList)
    </description>
    <schedule>every 12 hours synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=marksdb&endpoint=/_dr/task/tmchSmdrl&runInEmpty]]></url>
    <description>
      This job downloads the latest SMDRL from MarksDB and inserts it into the database.
      (See: TmchSmdrlServlet, SignedMarkRevocationList)
    </description>
    <schedule>every 12 hours from 00:15 to 12:15</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=marksdb&endpoint=/_dr/task/tmchCrl&runInEmpty]]></url>
    <description>
      This job downloads the latest CRL from MarksDB and inserts it into the database.
      (See: TmchCrlServlet)
    </description>
    <schedule>every 12 hours synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/syncGroupMembers&runInEmpty]]></url>
    <description>
      Syncs RegistrarContact changes in the past hour to Google Groups.
    </description>
    <schedule>every 1 hours synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=sheet&endpoint=/_dr/task/syncRegistrarsSheet&runInEmpty]]></url>
    <description>
      Synchronize Registrar entities to Google Spreadsheets.
    </description>
    <schedule>every 1 hours synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/task/resaveAllEppResourcesPipeline?fast=true]]></url>
    <description>
      This job resaves all our resources, projected in time to "now".
    </description>
    <schedule>1st monday of month 09:00</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/task/expandRecurringBillingEvents]]></url>
    <description>
      This job runs an action that creates synthetic OneTime billing events from Recurring billing
      events. Events are created for all instances of Recurring billing events that should exist
      between the RECURRING_BILLING cursor's time and the execution time of the action.
    </description>
    <schedule>every day 03:00</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/task/deleteExpiredDomains]]></url>
    <description>
      This job runs an action that deletes domains that are past their
      autorenew end date.
    </description>
    <schedule>every day 03:07</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/deleteProberData&runInEmpty]]></url>
    <description>
      This job clears out data from probers and runs once a week.
    </description>
    <schedule>every monday 14:00</schedule>
    <timezone>UTC</timezone>
    <target>backend</target>
  </cron>

  <!-- TODO: Add borgmon job to check that these files are created and updated successfully. -->
  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/exportReservedTerms&forEachRealTld]]></url>
    <description>
      Reserved terms export to Google Drive job for creating once-daily exports.
    </description>
    <schedule>every day 05:30</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/exportPremiumTerms&forEachRealTld]]></url>
    <description>
      Premium terms export to Google Drive job for creating once-daily exports.
    </description>
    <schedule>every day 05:00</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/readDnsQueue?jitterSeconds=45]]></url>
    <description>
      Lease all tasks from the dns-pull queue, group by TLD, and invoke PublishDnsUpdates for each
      group.
    </description>
    <schedule>every 1 minutes synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_ah/sessioncleanup?clear]]></url>
    <description>
      Delete up to 100 expired _ah_SESSION entities from Datastore.
    </description>
    <schedule>every 15 minutes</schedule>
    <target>backend</target>
  </cron>
</cronentries>
